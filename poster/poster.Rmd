---
main_topsize: 0.1 #percent coverage of the poster
main_bottomsize: 0.1
#ESSENTIALS
title: '**Bank Failure Prediction Models Addressing Imbalanced Data and Out-of-Time Performance**'
primary_colour: '#221132'
secondary_colour: '#FFFFFF'
author:
  - name: 'Seyma Gunonu'
    main: true
    twitter: seyma_gunonu
    email: 'seymagunonu@gmail.com'
  - name: 'Gizem Altun'
    main: true
    twitter: GizemAltn99
    email: 'gizemaltn99@gmail.com'
  - name: 'Mustafa Cavus'
    main: true
    twitter: mustafa__cavus
    email: 'mustafacavus@eskisehir.edu.tr'
affiliation:
    address: Department of Statistics, Eskisehir Technical University
main_findings:
  - "Tree-Based Approaches for **Bank Failure** Prediction: **Imbalanced Data** and **Out-of-Time** Scenarios."
logoleft_name: estu_logo.png
logoright_name: irsysc2023_logo.png
logocenter_name: qrcode.png
output: 
  posterdown::posterdown_betterport:
    self_contained: false
    pandoc_args: --mathjax
    number_sections: false
    fig_caption: true
bibliography: packages.bib
link-citations: true
knit: pagedown::chrome_print
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      tidy = FALSE,
                      message = FALSE,
                      fig.align = 'center',
                      out.width = "100%")
options(knitr.table.format = "html") 
```

```{=html}
<style>
#title {
  font-size: 110pt;
}

#references {
    font-size: 30px;
    line-height: 0.9;
}

</style>
```
```{=html}
<style>
  .header-logo {
    position: absolute;
    top: 0;
    right: 0;
  }
</style>
```
# Introduction

Banks are crucial to the financial system and must operate effectively. Recently, bank failure models have been used to predict the likelihood of bank failures by analyzing relevant metrics. In addition to ensuring high out-of-sample performance, it is important to address the poor out-of-time performance of bank failure prediction models to maintain their utility (Du Jardin & Séverin, 2011, Manthoulis et al., 2020). When using tabular data sets, deep learning models are used more than tree-based models (Carmona et al., 2019; Petropoulos et al., 2020; Grinsztajn et al., 2022). As a difference, it is being examined what kind of results such data sets may encounter with decision trees, random forests, and extra trees models.

```{r, include=FALSE}
knitr::write_bib(c('posterdown', 'rmarkdown','pagedown'), 'packages.bib')
```

```{r figures, fig.cap='Failed Banks in the U.S. by years'}
knitr::include_graphics("failed_banks_by_years.png")
```

The data has collected from the FDIC database using the **{fdicdata}** package in R (Dar & Pillmore, 2023). It covers information about whether banks were active or closed within 15-year period from 2008 to 2023. Three different variable groups which are detailed in **Figure 2**\@ref(fig:variable) were used, incorporating the **CAMELS indicators** (Capital, Asset Quality, Management Adequacy, Earnings, Liquidity, and Sensitivity to Market Risk) (Gogas et al., 2018; Petropoulos et al., 2020). When determining the time ranges in the data set, **in-sample** and **out-of-sample** were obtained between **2008-2014**, while models were builded using the **out-of-time** set between **2014-2023**. **Figure 1**\@ref(fig:figures) includes the banks that failed in the U.S. during these time ranges.

```{r, variable, out.width="75%", fig.cap='Details of Variables'}
knitr::include_graphics("variable_groups.png")
```

# Methods

Three different models were employed in this study because they provides varying variance of predictions. When comparing the prediction variances of these three methods, it becomes apparent that the **Decision Trees** yields high-variance predictions, the **Random Forests** provides predictions with moderate variance, and the **Extra Trees** generate predictions with low variance (Gogas et al., 2018). The structures of Decision trees, Random forests and Extra trees models are shown in **Figure 3**\@ref(fig:rf), and **Figure 4**\@ref(fig:ext). Random Forests lies in aggregating predictions generated by multiple decision trees. Breiman improved upon the overfitting-prone CART method by introducing Random Forests, an extension of bagging trees. It differs by using feature subsets for each tree, reducing correlation. This added randomness enhances stability and generalization, making Random Forests valuable across applications (Breiman, 2000).

```{r, rf, fig.cap='Random Forests Structure'}
knitr::include_graphics("random_forests_structure.png")
```

Its two primary distinctions from Random Forests are that it splits nodes by randomly selecting cut-points and that it grows the trees using the entire learning sample (Geurts et al., 2006).

```{r, ext, fig.cap='Extra Trees Structure'}
knitr::include_graphics("extra_trees_structure.png")
```

```{r, resampling,out.width="50%",fig.cap='Resampling Techniques’ Structures'}
knitr::include_graphics("resampling_methods.png")
```

To address imbalanced data, resampling techniques like **undersampling** reduce majority class samples, **oversampling** increases minority class samples, and **SMOTE** creates synthetic minority samples for better representation in **Figure 5**\@ref(fig:resampling).

# Results

Three different variable groups were considered, and three different models were applied. The dataset exhibited an imbalance between the classes. So, imbalance in the data was addressed using various resampling methods. **Accuracy** and **F1 scores** were calculated for each variable groups to assess model performance. The results of out-of-sample showed that generally the weighted-based which is cost-sensitive method had the highest accuracy for all variable groups, while SMOTE had the lowest accuracy. Accuracy values were closer between variable groups in the out-of-time strategy, and some results were the same in random forests and extra trees. Variable groups with under-sampling had lower F1 values in the out-of-time strategy. As a result, **the choice of resampling method's effectiveness varied depending on the variable group and model.** **The out-of-time strategy is important to assess how models perform with changing data over time.**

```{r, fig.cap='Out-of-Sample Results'}
knitr::include_graphics("out_of_sample_results.png")
```

```{r, fig.cap='Out-of-Time Results'}
knitr::include_graphics("out_of_time_results.png")
```

# Conclusion

In study focuses on using Decision Trees, Random Forests, and Extra Trees to make bank failure predictions. What makes this study apart is the usage of a **1-year lag (t-1) period** in the dataset. When examining the overall results of the models, **it was observed that Random Forests and Extra Trees yielded similar and high-quality results, making them the most effective models for predicting bank failures.**

# References

Gogas, P., Papadimitriou, T., & Agrapetidou, A. (2018). Forecasting bank failures and stress testing: A machine learning approach. Int. J. Forecast., 34(3), 440-455.

Petropoulos, A., Siakoulis, V., Stavroulakis, E., & Vlachogiannakis, N. E. (2020). Predicting bank insolvencies using machine learning techniques. Int. J. Forecast., 36(3), 1092-1113.

Dar, U., & Pillmore, B. (2023). fdicdata: Accessing FDIC Bank Data. R package version 0.1.0.

Carmona, P., Climent, F., & Momparler, A. (2019). Predicting failure in the US banking sector: An extreme gradient boosting approach. Int. Rev. Econ. Finance, 61, 304-323.

Grinsztajn, L., Oyallon, E., & Varoquaux, G. (2022). Why do tree-based models still outperform deep learning on typical tabular data?. Adv. Neural Inf. Process, 35, 507-520.

Breiman, L. (2000). Some infinity theory for predictor ensembles. Technical Report 579, Statistics Dept. UCB.

Geurts, P., Ernst, D., & Wehenkel, L. (2006). Extremely randomized trees. Machine learning, 63, 3-42.
